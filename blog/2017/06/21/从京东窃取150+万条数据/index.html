<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>从京东&quot;窃取&quot;150+万条数据 - 公子世无双，陌上人如玉</title>
    <meta charset="utf-8" />
    <meta name="author" content="" />
    <meta name="description" content="An spider to crawl jindong item and comments" />
    <meta name="keywords" content="python,spider" />
    <link rel="stylesheet" href="/media/css/main.css" type="text/css">
    <link rel="stylesheet" href="/media/css/prettify.css" type="text/css">
  </head>
  <body class="container">
    <div>
      <header class="masthead">
        <h1 class="masthead-title"><a href="/">公子世无双，陌上人如玉</a></h1>
        <p>凡走过,必留下痕迹</p>
        <ul>
          <li><a href="/blog/">Blog</a></li>
          <li><a href="/tags/">Tags</a></li>
          <li><a href="/about/">About</a></li>
          <li><a href="https://github.com/samrayleung">GitHub</a></li>
          <li><a href="/rss.xml">RSS</a></li>
        </ul>
        <form method="get" id="searchform" action="//www.google.com/search">
          <input type="text" class="field" name="q" id="s" placeholder="Search">
          <input type="hidden" name="as_sitesearch" value="https//samrayleung.github.io">
        </form>
      </header>
    </div>

<div>
<div class="post">
<h1>从京东&quot;窃取&quot;150+万条数据</h1>
<p>
笔者最近编写了两只京东商品和评论的分布式爬虫来进行数据分析，现在就来分享一下。
</p>
<div id="outline-container-org4470b52" class="outline-2">
<h2 id="org4470b52">爬取策略</h2>
<div class="outline-text-2" id="text-org4470b52">
<p>
众所周知，爬虫比较难爬取的就是动态生成的网页，因为需要解析 JS, 其中比较典型的
例子就是淘宝，天猫，京东，QQ 空间等。所以在笔者爬取京东网站的时候，首先需要确
定的就是爬取策略。因为笔者想要爬取的是商品的信息以及相应的评论，并没有爬取特定
的商品的需求。所以在分析京东的网页的 url 的时候, 决定使用类似全站爬取的策略。
分析如图：
</p>


<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_analyze.png" alt="jd_analyze.png" />
</p>
</div>

<p>
可以看出，京东不同的商品类别是对应不同的子域名的，例如 <code>book</code> 对应的是图书，
<code>mvd</code> 对应的是音像， <code>shouji</code> 对应的是手机等。因为笔者使用的是获取 <code>&lt;a href&gt;</code>
标签里面的 url 值，然后迭代爬取的策略。所以要把爬取的 url 限定在域名为
<code>jd.com</code> 范围内，不然就有可能会出现无限广度。此外，有相当多的页面是不会包含商
品信息的；例如： <code>help.jd.com</code>, <code>doc.jd.com</code> 等，因此使用 <code>jd.com</code> 这个域名范
围实在太大了，所以把所需的子域名都添加到一个 list :
</p>
<div class="org-src-container">
<pre class="src src-python">jd_subdomain = ["jiadian", "shouji", "wt", "shuma", "diannao",
		"bg", "channel", "jipiao", "hotel", "trip",
		"ish", "book", "e", "health", "baby", "toy",
		"nong", "jiu", "fresh", "china", "che", "list"]
</pre>
</div>
</div>
</div>
<div id="outline-container-org90aa04c" class="outline-2">
<h2 id="org90aa04c">提取数据</h2>
<div class="outline-text-2" id="text-org90aa04c">
<p>
在确定了爬取策略之后，爬虫就可以不断地进行工作了。那么爬虫怎么知道什么时候才是
商品信息的页面呢？再来分析一下京东的商品页面：
</p>


<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_item_analyze.png" alt="jd_item_analyze.png" />
</p>
</div>

<p>
从上面的信息可以看出，每个商品的页面都是以 <b>item.jd.com/xxxxxxx.html</b> 的形式存
在的；而 xxxxxxx 就是该商品的 sku-id. 所以只需对 url 进行解析，子域名为 <code>item</code>
即商品页面，就可以进行爬取。页面提取使用 Xpath 即可，也无需赘言。不过，需要注
意的是对商品而言，非常重要的价格就不是可以通过爬取 HTML 页面得到的。因为价格是
经常变动的，所以是异步向后台请求的。对于这些异步请求的数据，打开控制台，然后刷
新，就可以看到一堆的 JS 文件，然后寻找相应的请求带有 "money 或者price" 之类关
键字的 JS 文件，应该就能找到。如果还没办法找出来的话，Firefox 上有一个
<a href="https://addons.mozilla.org/en-US/firefox/addon/user-agent-switcher/">user-agent-switcher</a> 的扩展，然后通过这个扩展把自己的浏览器伪装成 IE6, 相信所有
花俏的 JS 都会没了, 只剩下那些不可或缺的 JS, 这样结果应该一目了然了，这么看来
IE6 还是有用滴。最终找到的URL 如下
<b><a href="https://p.3.cn/prices/mgets?callback=jQuery6646724&amp;type=1&amp;area=19_1601_3633_0.137875165&amp;pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg%2F6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN%2FxOch3MpwWpHICu4P%0AVcgcOm11GQ%3D%3D&amp;pduid=14966417675252009727775&amp;pdpin=%E5%85%91%E9%87%91%E8%BE%B0%E6%89%8B&amp;pdbp=0&amp;skuIds=J_3356012&amp;ext=10000000&amp;source=item-pc">https://p.3.cn/prices/mgets?callback=jQuery6646724&amp;type=1&amp;area=19_1601_3633_0.137875165&amp;pdtk=9D4RIAHY317A3bZnQNapD7ip5Dg/6NXiIXt90Ahk0if2Yyh39PZQCuDBlhN/xOch3MpwWpHICu4P
VcgcOm11GQ==&amp;pduid=14966417675252009727775&amp;pdpin=兑金辰手&amp;pdbp=0&amp;skuIds=J_3356012&amp;ext=10000000&amp;source=item-pc</a></b>
</p>

<p>
不得不说，URL 实在是太长了。根据经验，大部分的参数应该都是没什么用的，应该可以
去掉的，所以在浏览器就一个个参数去掉，然后试试请求是否成功，如果成功，说明此参
数无关重要，最后简化成： <code>http://p.3.cn/prices/mgets?pduid={}&amp;skuIds=J_{}</code>
sku_id 即商品页面的 URL中包含的数字，而 pduid 则是一随机整数而已，用
<code>random.randint(1, 100000000)</code> 函数解决。
</p>
</div>
</div>
<div id="outline-container-orgebf1bd7" class="outline-2">
<h2 id="orgebf1bd7">商品评论</h2>
<div class="outline-text-2" id="text-orgebf1bd7">
<p>
商品的评论也是以 sku-id 为参数通过异步的方式进行请求的，构造请求的方法跟价格类
似，也不需过多赘述。只是想要吐嘈一下的是，京东的评论是只能一页页向后翻的，不能
跳转。还有一点就是，即使某样商品有 10+w 条评论，最多也只是返回 100 页的数据。
略坑
</p>
</div>
</div>
<div id="outline-container-org3e62992" class="outline-2">
<h2 id="org3e62992">反爬虫策略</h2>
<div class="outline-text-2" id="text-org3e62992">
<p>
商品的爬取策略以及提取策略都确定了，一只爬虫就基本成型了。但是一般比较大型的网
站都有反爬虫措施的。所以道高一尺，魔高一丈，爬虫也要有对应的反 反爬虫策略
</p>
</div>
<div id="outline-container-orge3f6d89" class="outline-3">
<h3 id="orge3f6d89">禁用 cookie</h3>
<div class="outline-text-3" id="text-orge3f6d89">
<p>
通过禁用 cookie, 服务器就无法根据 cookie 判断出爬虫是否访问过网站
</p>
</div>
</div>
<div id="outline-container-org961359f" class="outline-3">
<h3 id="org961359f">轮转 user-agent</h3>
<div class="outline-text-3" id="text-org961359f">
<p>
一般的爬虫都会使用浏览器的 user-agent 来模拟浏览器以欺骗服务器 (当然，如果你
是一只什么 user-agent都不用耿直的小爬虫，笔者也无话可说). 为了提高突破反爬虫
策略的成功率，可以定义多个 user-agent, 然后每次请求都随机选择 user-agent。
</p>
</div>
</div>
<div id="outline-container-orge979e38" class="outline-3">
<h3 id="orge979e38">伪装成搜索引擎</h3>
<div class="outline-text-3" id="text-orge979e38">
<p>
要说最著名的爬虫是谁？肯定是搜索引擎，它本质上也是爬虫，而且是非常强大的爬虫。
而且这些爬虫可以光明正大地去爬取各式网站，相信各式网站也很乐意被它爬。那么，
现在可以通过修改 user-agent 伪装成搜索引擎，然后再结合上面的轮转 user-agent,
伪装成各式搜索引擎：
</p>
<pre class="example">
'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)',
'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)',
'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)',
'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)',
'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)',
</pre>
</div>
</div>
<div id="outline-container-orgac831b2" class="outline-3">
<h3 id="orgac831b2">代理 IP</h3>
<div class="outline-text-3" id="text-orgac831b2">
<p>
虽说可以伪装成搜索引擎，但是因为 http 请求是建立在三次握手之上的，爬虫的 IP
还是会被记录下来的，如果同一个 IP 访问得太频繁，那基本就可以确定是一只爬虫了，
然后就把它的 IP 封掉，温和一点的就会叫你输入验证码，不然就返回 403. 对待这种
情况，就需要使用代理 IP 了。只是代理 IP 都有不同程度的延迟，并且免费的 IP 大
多不能用，所以这是不得而为之了
</p>
</div>
</div>
</div>
<div id="outline-container-org53dfc95" class="outline-2">
<h2 id="org53dfc95">扩展成分布式爬虫</h2>
<div class="outline-text-2" id="text-org53dfc95">
<p>
一台机器的爬虫可能爬取一个网站可能需要 100 天，而且带宽也到达瓶颈了，那么是否
可以提高爬取效率呢？那就用 100 台机器，1天应该就能爬取完 (当然，现实并非如此美
好). 这个就涉及到分布式的爬虫的问题。而不同的分布式爬虫有不同的实现方法，而笔
者选择了 scrapy 和 redis 整合的 <a href="https://github.com/rolando/scrapy-redis">scrapy-redis</a> 来实现分布式，URL 的去重以及调度
都有了相应的实现了，也无需额外的操心
</p>
</div>
</div>
<div id="outline-container-org4f861a7" class="outline-2">
<h2 id="org4f861a7">爬虫监控</h2>
<div class="outline-text-2" id="text-org4f861a7">
<p>
既然爬虫从单机变成了分布式，新的问题随之而来：如何监控分布式爬虫呢？在单机的时
候，最简单的监控 -- 直接将爬虫的日志信息输出到终端即可。但是对于分布式爬虫，这
样的做法显然不现实。笔者最终选择使用 <a href="https://graphiteapp.org">graphite</a> 这个监控工具。
</p>
</div>
<div id="outline-container-org8c7e2b3" class="outline-3">
<h3 id="org8c7e2b3">scrapy-graphite</h3>
<div class="outline-text-3" id="text-org8c7e2b3">
<p>
参考 Github上 <a href="https://github.com/gnemoug/distribute_crawler">distributed_crawler</a> 的代码，将单机版本的 <a href="https://github.com/noplay/scrapy-graphite">scrapy-graphite</a> 扩展成
基于分布式的 graphite 监控程序，并且实现对 python3 的支持。
</p>
</div>
</div>
<div id="outline-container-org7b23a8a" class="outline-3">
<h3 id="org7b23a8a">docker</h3>
<div class="outline-text-3" id="text-org7b23a8a">
<p>
但是 graphite 只是支持 python2, 并且安装过程很麻烦，笔者在折腾大半天后都无法
安装成功，实在有点沮丧。最后想起了伟大的 docker, 并且直接找到已经打包好的
image. 数行命令即解决所有的安装问题，不得不说：docker, 你值得拥有。运行截图：
</p>


<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_comment_graphite1.png" alt="jd_comment_graphite1.png" />
</p>
</div>



<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_comment_graphite2.png" alt="jd_comment_graphite2.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbdeddde" class="outline-2">
<h2 id="orgbdeddde">爬虫拆分</h2>
<div class="outline-text-2" id="text-orgbdeddde">
<p>
本来爬取商品信息的爬虫和爬取评论的爬虫都是同一只爬虫，但是后来发现，再不使用代
理 IP 的情况下，爬取到 150000 条商品信息的时候，需要输入验证码。但是爬取商品评
论的爬虫并不存在被反爬策略限制的情况。所以我将爬虫拆分成两只爬虫，即使无法爬取
商品信息的时候，还可以爬取商品的评论信息。
</p>
</div>
</div>
<div id="outline-container-org94e34be" class="outline-2">
<h2 id="org94e34be">小结</h2>
<div class="outline-text-2" id="text-org94e34be">
<p>
在爬取一天之后，爬虫成果：
</p>
</div>
<div id="outline-container-orgfc301f5" class="outline-3">
<h3 id="orgfc301f5">评论</h3>
<div class="outline-text-3" id="text-orgfc301f5">

<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_comment.png" alt="jd_comment.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-orgd875d16" class="outline-3">
<h3 id="orgd875d16">评论总结</h3>
<div class="outline-text-3" id="text-orgd875d16">

<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_comment_summary.png" alt="jd_comment_summary.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-orgb85bd66" class="outline-3">
<h3 id="orgb85bd66">商品信息</h3>
<div class="outline-text-3" id="text-orgb85bd66">

<div class="figure">
<p><img src="/assets/blog/2017/06/21/从京东窃取150+万条数据/jd_parameters.png" alt="jd_parameters.png" />
</p>
</div>

<p>
商品信息加上评论数约 150+w.
</p>
</div>
</div>
</div>
<div id="outline-container-org7ea8a7e" class="outline-2">
<h2 id="org7ea8a7e">参考及致谢</h2>
<div class="outline-text-2" id="text-org7ea8a7e">
<ul class="org-ul">
<li><a href="https://github.com/noplay/scrapy-graphite">https://github.com/noplay/scrapy-graphite</a></li>
<li><a href="https://github.com/gnemoug/distribute_crawler">https://github.com/gnemoug/distribute_crawler</a></li>
<li><a href="https://github.com/hopsoft/docker-graphite-statsd">https://github.com/hopsoft/docker-graphite-statsd</a></li>
</ul>
</div>
</div>
<div id="outline-container-org3bf13a5" class="outline-2">
<h2 id="org3bf13a5">项目源码</h2>
<div class="outline-text-2" id="text-org3bf13a5">
<p>
<a href="https://github.com/samrayleung/jd_spider">https://github.com/samrayleung/jd_spider</a>
</p>
</div>
</div>

</div>
</div>
    <div>
      <div class="post-meta">
        <span title="post date" class="post-info">2017-06-21</span>
        <span title="last modification date" class="post-info">2017-06-21</span>
        <span title="tags" class="post-info"><a href="/tags/python/">python</a></span>
        <span title="author" class="post-info"></span>
      </div>
      <section>
        <h1>Comments</h1>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          //var disqus_developer = 1;
          var disqus_identifier = "/blog/2017/06/21/从京东窃取150+万条数据";
          var disqus_url = "http://https//samrayleung.github.io/blog/2017/06/21/从京东窃取150+万条数据";
          var disqus_shortname = 'Samray';
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="//disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </section>
      <script src="//code.jquery.com/jquery-latest.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/prettify.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="/media/js/main.js"></script>
      <div class="footer">
        <p>Generated by <a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.x (<a href="http://orgmode.org">Org mode</a> 9.x)</p>
        <p>
          Copyright &copy; 2012 - <span id="footerYear"></span> <a href="mailto:samray &lt;at&gt; workstation"></a>
          &nbsp;&nbsp;-&nbsp;&nbsp;
          Powered by <a href="https://github.com/kelvinh/org-page" target="_blank">org-page</a>
          <script type="text/javascript">document.getElementById("footerYear").innerHTML = (new Date()).getFullYear();</script>
        </p>
      </div>
    </div>

  </body>
</html>
